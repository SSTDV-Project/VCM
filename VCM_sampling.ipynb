{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "gpu_idx= '3'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= gpu_idx\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "import yaml\n",
    "import nibabel as nib\n",
    "\n",
    "from monai import transforms\n",
    "from generative.networks.schedulers import DDIMScheduler\n",
    "from generative.networks.nets import DiffusionModelUNet, AutoencoderKL\n",
    "\n",
    "from model.vcm import VCM\n",
    "from config.model_config import defaultCFG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:25<00:00,  1.93it/s]\n"
     ]
    }
   ],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "        transforms.LoadImaged(keys=[\"image\"]),\n",
    "        transforms.EnsureChannelFirstd(keys=[\"image\"], channel_dim=\"no_channel\"),\n",
    "        transforms.EnsureTyped(keys=[\"image\"]),\n",
    "        transforms.Orientationd(keys=[\"image\"], axcodes=\"LAS\"), # torch.Size([240, 240, 155])\n",
    "        transforms.AsDiscreted(keys=['image'], to_onehot=9)\n",
    "    ] \n",
    ")\n",
    "\n",
    "newinput_list = glob.glob('./data/*/new_semantics.nii.gz')\n",
    "\n",
    "for newinput_path in tqdm(newinput_list):\n",
    "    PREFIX = '/'.join(newinput_path.split('/')[:-1])\n",
    "    d= {'image':newinput_path}\n",
    "\n",
    "    d = train_transforms(d)\n",
    "    img = d['image']\n",
    "\n",
    "    img = img[1:, ...]\n",
    "    \n",
    "    torch.save(img, rf'{PREFIX}/new_semantics.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NiftiSaver:\n",
    "    def __init__(self, output_dir: str) -> None:\n",
    "        super().__init__()\n",
    "        self.output_dir = output_dir\n",
    "        self.affine = np.array(\n",
    "            [\n",
    "                [-1.0, 0.0, 0.0, 96.48149872],\n",
    "                [0.0, 1.0, 0.0, -141.47715759],\n",
    "                [0.0, 0.0, 1.0, -156.55375671],\n",
    "                [0.0, 0.0, 0.0, 1.0],\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self.set_output_dir(self.output_dir)\n",
    "        \n",
    "    def set_output_dir(self, output_dir):\n",
    "        self.output_dir = output_dir\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "\n",
    "    def save(self, image_data: torch.Tensor, file_name: str, is_label=False) -> None:\n",
    "        image_data = image_data.cpu().numpy()\n",
    "        image_data = image_data[0, 0, ...]\n",
    "        if is_label:\n",
    "            image_data = image_data.astype(np.uint8)  \n",
    "\n",
    "        empty_header = nib.Nifti1Header()\n",
    "        sample_nii = nib.Nifti1Image(image_data, self.affine, empty_header)\n",
    "        nib.save(sample_nii, f\"{str(self.output_dir)}/{file_name}.nii.gz\")\n",
    "        \n",
    "    def save_label(self, img_path, file_name: str) -> None:\n",
    "        img_nib = nib.load(img_path)\n",
    "        empty_header = nib.Nifti1Header()\n",
    "        sample_nii = nib.Nifti1Image(np.array(img_nib.dataobj), self.affine, empty_header)\n",
    "        nib.save(sample_nii, f\"{str(self.output_dir)}/{file_name}.nii.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda setup\n",
      "\tUsing cuda: 3\n",
      "model construction, load the weights\n",
      "\t AE done\n",
      "\t Diffusion done\n"
     ]
    }
   ],
   "source": [
    "cfg = defaultCFG()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('cuda setup')\n",
    "print(f\"\\tUsing {device}: {gpu_idx}\")\n",
    "\n",
    "print('model construction, load the weights')\n",
    "autoencoder = AutoencoderKL(**cfg.get_AE_CFG())\n",
    "AE_weight_path = 'weights/autoencoder.pth'\n",
    "autoencoder.load_state_dict(torch.load(AE_weight_path))\n",
    "autoencoder.to(device)\n",
    "print(f\"\\t AE done\")\n",
    "\n",
    "diffusion = DiffusionModelUNet(**cfg.get_DM_CFG())\n",
    "Diff_weight_path = 'weights/diffusion_model.pth'\n",
    "diffusion.load_state_dict(torch.load(Diff_weight_path))\n",
    "diffusion.to(device)\n",
    "print(f\"\\t Diffusion done\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3500\n",
      "/root/vcm/VCM/out/data500/newSemantics/VCM/2024-09-02/log/3500\n",
      "\t VCM done\n",
      "0.896264910697937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]/opt/conda/envs/vcm/lib/python3.10/site-packages/torch/nn/modules/conv.py:605: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv3d(\n",
      "/opt/conda/envs/vcm/lib/python3.10/site-packages/torch/_tensor.py:1443: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  ret = func(*args, **kwargs)\n",
      "100%|██████████| 50/50 [38:46<00:00, 46.54s/it]\n"
     ]
    }
   ],
   "source": [
    "val_scheduler = DDIMScheduler(num_train_timesteps=1000, schedule=\"scaled_linear_beta\", beta_start=0.0015, beta_end=0.0205, clip_sample=False)\n",
    "val_scheduler.set_timesteps(num_inference_steps=200)\n",
    "\n",
    "src_path = 'data/*'\n",
    "val_data_list = glob.glob(src_path)\n",
    "\n",
    "VCM_enc_CFG, enc_CFG = cfg.get_VCM_enc_CFG()\n",
    "vcm = VCM(out_dim=3, diff_CFG=VCM_enc_CFG, enc_CFG=enc_CFG)\n",
    "VCM_weight_path = f'weights/vcm_wegith_complexSemantics.pt'\n",
    "acceler_dict = torch.load(VCM_weight_path)\n",
    "new_d={}\n",
    "for old_key in acceler_dict.keys():\n",
    "    if 'module.' in old_key:\n",
    "        new_key = old_key.replace('module.', '')\n",
    "    new_d[new_key] = acceler_dict[old_key]\n",
    "    \n",
    "del acceler_dict\n",
    "vcm.load_state_dict(new_d, strict=False)\n",
    "vcm.to(device)\n",
    "print(f\"\\t VCM done\")\n",
    "\n",
    "scale_factor = 0.8962649106979370\n",
    "print(scale_factor)\n",
    "\n",
    "saver = NiftiSaver(f'generated/VCM_complexSemantics')\n",
    "\n",
    "\n",
    "for path in tqdm(val_data_list):\n",
    "\n",
    "    sid = path.split('/')[-1]\n",
    "    \n",
    "    d = {\n",
    "            'seg':torch.load(f'{path}/new_semantics.pt')\n",
    "            \n",
    "            }\n",
    "\n",
    "    cond = torch.load(f'{path}/sex-age-ventV-brainV.pt')\n",
    "    val_cond = cond.unsqueeze(0)\n",
    "    \n",
    "    val_label = d['seg'] \n",
    "    val_label = val_label.unsqueeze(0)\n",
    "    \n",
    "    val_BZ = val_label.shape[0]\n",
    "\n",
    "    noise = torch.randn((val_BZ, 3, 20, 28, 20))\n",
    "    noise = noise.to(device)\n",
    "\n",
    "    image = noise\n",
    "    image4LDM = noise.detach().clone()\n",
    "    \n",
    "    val_cond = val_cond.view(val_BZ, 1, 4).to(device)\n",
    "    val_cond_concat = val_cond.view(val_BZ, 4, 1, 1, 1).to(device)\n",
    "    val_cond_concat = val_cond_concat.expand(list(val_cond_concat.shape[0:2]) + list(image.shape[2:]))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with autocast():\n",
    "            progress_bar = val_scheduler.timesteps\n",
    "            for t in progress_bar:\n",
    "                \n",
    "                timesteps = torch.Tensor((t,)).to(device).long()\n",
    "                \n",
    "                # contorlled by VCM\n",
    "                epsilon = diffusion(torch.cat((image, val_cond_concat), dim=1),\n",
    "                                        timesteps=timesteps,\n",
    "                                        context=val_cond,)\n",
    "                scale, shift = vcm(x=torch.cat([image, epsilon], dim=1),\n",
    "                                    y=val_label.to(device),\n",
    "                                    timesteps=timesteps)\n",
    "                vcm_out = epsilon * (1+scale) + shift\n",
    "                image, _ = val_scheduler.step(vcm_out, t, image)\n",
    "                \n",
    "                # uncontrolled (BrainLDM)\n",
    "                ldm_out = diffusion(torch.cat((image4LDM, val_cond_concat), dim=1),\n",
    "                                        timesteps=timesteps,\n",
    "                                        context=val_cond,)\n",
    "                image4LDM, _ = val_scheduler.step(ldm_out, t, image4LDM)\n",
    "                \n",
    "            # decode to MRI\n",
    "            vcm_sample = autoencoder.decode_stage_2_outputs(image.to(device)/scale_factor)\n",
    "            ldm_sample = autoencoder.decode_stage_2_outputs(image4LDM.to(device)/scale_factor)\n",
    "            saver.save(vcm_sample, f'{sid}__2-VCM')\n",
    "            saver.save(ldm_sample, f'{sid}__1-LDM')\n",
    "            \n",
    "    saver.save_label(f'{path}/T1.nii.gz', f'{sid}__0-ori_T1')\n",
    "    saver.save_label(f'{path}/new_semantics.nii.gz', f'{sid}__3-input_seg')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gene",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
